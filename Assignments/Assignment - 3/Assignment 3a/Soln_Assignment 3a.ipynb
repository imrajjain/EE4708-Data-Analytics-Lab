{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np               # for mathematical operations, for arrays\n",
    "import pandas as pd              # Used for different data manipulation tasks\n",
    "import matplotlib.pyplot as plt  # Used for Plotting \n",
    "import random                    # For random number generator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')# To avoid warnings note while running the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : Implementing Gradient Descent\n",
    "\n",
    "Gradient Descent is an first order optimization algorithm to find the local minimum differentiable function (usually a loss function or an objective function). We start with a random point on the function and move in the negative direction of the gradient of the function to reach the find minimum.\n",
    "\n",
    "Let $F(x)$ be a differentiable function parameterised by $x$ and $x^*$ be the parameter value which minimises $F(x)$. The initial value is $x_0=a_0$. To determine $x^*$ using gradient descent, at each of the $i^{th}$ the parameter updation rule is:\n",
    "\n",
    "\\begin{align*}\n",
    "x_i = x_{i-1} - \\eta \\nabla F(a_{i-1}), \\hspace{1cm} i\\geq 1\n",
    "\\end{align*}  \n",
    "where $x_i$ is updated value of the parameter in the $i^{th}$ iteration. \n",
    "\n",
    "\\begin{align*}\n",
    "\\eta \\nabla F(a_{i-1}) = \\eta \\frac{dF}{dx}\\bigr \\rvert_{x=a_{i-1}}\n",
    "\\end{align*} \n",
    "\n",
    "**Stopping Criteria**: Following are three ways which are used as stopping criteria in gradient descent algorithm:\n",
    "1. Limiting number of iterations\n",
    "2. Threshold for change in the parameter (i.e difference between the updated parameter and parameter in the previous iteration) --> Change in parameter > Threshold\n",
    "3. Threshold for change in gradient step\n",
    "                  \n",
    "**Question** : Find $x^*$ which minimises $F(x) = (x+5)^2$ using gradient descent \n",
    "\n",
    "**Note : Few variables are set to a given value, read the instructions in the pseudo code**\n",
    "\n",
    "**Pseudo Code**:\n",
    "1. Initialise the parameter ('x') with some random number,set learning rate = 0.01(section 4 has details on \"how to select appropriate learning rate for a given problem\"), set threshold = 0.000001,\n",
    "2. Define a function for calculating differential of F (Hint : Use lambda method for single line of code)\n",
    "3. Compute the updated value of the parameter using the formula: $x_i = x_i-1 - \\eta \\nabla F(a_{i-1})$ \n",
    "4. Iterate over step 3 until the stopping criteria is satisfied (You have to choose stopping criteria appropriately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Implementing Code\n",
    "Implement the above pseudo code and print the optimum value of $x$ and $F(x)$. Also print the number of iterations required to reach that optimum.\n",
    "\n",
    "**Note : Carefully choose your stopping criteria for reaching optimum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "attachments": {
    "Normal%20Equation.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : Linear Regression - Finding the Regression parameters using Normal Equation (Analytical Approach)\n",
    "**Dataset** : dataset1.csv (given)\n",
    "\n",
    "Given a dataset (here dataset1.csv), find the coefficients (parameters) using the Normal Equation and predict the outputs for the same dataset (dataset1.csv). Given input features in matrix $X$ and observed output or target vector $y$, the regression parameters are given by the equation:\n",
    "![Normal%20Equation.png](attachment:Normal%20Equation.png)\n",
    "\n",
    "\n",
    "**Pseudo Code:**\n",
    "1. Append ones to X, let say that as \"X_appended\" (This is to be done when there is intercept, if you know before hand that intercept is zero, then this step can be ignored)\n",
    "2. Find the pseudo Inverse of the X_appended\n",
    "3. Find the dot product of transpose of X_appended and target vector (y),\n",
    "4. Find \"theta\" (parameters) using \"Normal Equation\" given above (i.e Matrix multiplication of outputs of Step 2 and Step 3)\n",
    "5. Predict ouputs for X_new by appending it with ones and then using the formula $y=X_{new}\\theta$\n",
    "\n",
    "**Note : When you perform matrix multiplications or dot products on vectors or matrices, always keep track of the matrix or vectors shapes. This helps you in avoiding implementation errors.**\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "**Implementation Note:** Step 1 is crucial. We store each example as a row in the the $X$ matrix in Python `numpy`. To take into account the intercept term ($\\theta_0$), we add an additional first column to $X$ and set it to all ones. This allows us to treat $\\theta_0$ as simply another 'feature'.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import X, y from \"dataset1.csv\"\n",
    "# X and y are numpy array, Hint : store the data in the pandas dataframe and then convert it to numpy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Function to predict the outputs\n",
    "Implement the predict function below. Predict the outputs for the given data (dataset1.csv), plot the predicted outputs Vs input features and scatter plot for dataset1.csv (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    Predictions of X for a given theta.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with size of (m x n)\n",
    "    \n",
    "    theta : array with size of (n+1, 1).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    predictions : array of size (m x 1)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# Your predictions for the dataset1.csv\n",
    "\n",
    "\n",
    "\n",
    "# plot a line Predictions Vs features and scatter plot for the training data X,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 : Linear Regression - Find the Regression parameters using Gradient Descent\n",
    "This section is about applying gradient descent algorithm to find theta at which a cost function (Mean Squared Error) is minimum. This can be done using the gradient descent algorithm discussed in Section 1, where $x$ is replaced by the regression parameters. \n",
    "\n",
    "Refer to pseudo code in Section 1 for gradient descent algorithm. In simple terms, we initialise the regression parameters with some random number and we compute the gradient step to update the regression parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Import and visualise the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from dataset1.csv - Same as Section 2\n",
    "# Import X and y as numpy array, Hint : store the data in the pandas dataframe and then convert it to numpy array\n",
    "\n",
    "\n",
    "# Scatter Plot the data X and y for visualisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Compute Cost Function - Mean Squared Error (MSE) \n",
    "The cost function used is Mean Squared Error (MSE) represented as $J(\\theta)$ and is given by:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n",
    "\n",
    "where the hypothesis $h_\\theta(x)$ is a linear model given by: \n",
    "$$ h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1 x_1$$\n",
    "\n",
    "As you perform gradient descent to learn minimize the cost function $J(\\theta)$, it is helpful to monitor the convergence by computing the cost. This way you can know whether you choose high learning rate or low learning rate. Suppose if your learning rate is too high, you can see your cost will go up and down, but will not converge. If you choose good learning rate, then you can see your cost going down smoothly and converges.\n",
    "\n",
    "In this you will implement a function to calculate $J(\\theta)$ so you can check the convergence of your gradient descent implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute cost for linear regression. Computes the cost of using theta as the\n",
    "    parameter for linear regression to fit the data points in X and y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with the shape of (m x n+1)\n",
    "        where m is the number of examples,n is the number of features\n",
    "        n+1 == n features + 1 for bias term(intercept as a feature)\n",
    "    \n",
    "    y : array with the shape of (m,1)\n",
    "    \n",
    "    theta : array with the shape of (n+1,1)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float -> The value of the regression cost function.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Write your code here\n",
    "    \n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Implementation of Gradient Descent \n",
    "\n",
    "Recall that the parameters of your model are the $\\theta_j$ values. These are\n",
    "the values you will adjust to minimize cost $J(\\theta)$. One way to do this is to\n",
    "use the batch gradient descent algorithm. In batch gradient descent, each\n",
    "iteration performs the update\n",
    "\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} \\qquad \\text{simultaneously update } \\theta_j \\text{ for all } j$$\n",
    "\n",
    "With each step of gradient descent, your parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost J($\\theta$).\n",
    "\n",
    "\n",
    "**Important implementation notes**\n",
    "\n",
    "The function `gradientDescent` calls `computeCost` on every iteration and saves the cost to a python list. If you have implemented gradient descent and `computeCost` correctly, your value of $J(\\theta)$ should never increase, and should converge to a steady value by the end of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X_appended, y, theta, alpha, precision = 0.001):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn `theta`. Updates theta by taking `num_iters`\n",
    "    gradient steps with learning rate `alpha`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_appended : array with shape of (m x n+1). Note : n+1 = n features + 1 intercept\n",
    "    \n",
    "    y : array with shape of (m, 1)\n",
    "    \n",
    "    theta : array with shape of (n+1,1)\n",
    "    \n",
    "    alpha : float value, called as \"learning rate\"\n",
    "    \n",
    "    precision : float, (One of the Stopping criteria). This is compared with the change in the cost from previous iter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : array wih shape of (n+1,1),The learned linear regression parameters\n",
    "    \n",
    "    J_history : A python list for the values of the cost function after every iteration. This is to check for convergence\n",
    "    \n",
    "    Count : Integer, Number of iterations taken to converge\n",
    "    \n",
    "    Cost : Float, Mean squared error at the end of the iteration\n",
    "    \n",
    "    \n",
    "    Hint: \n",
    "    ------------\n",
    "    1. Peform a single gradient step on the parameter vector theta.\n",
    "    2. Loop over the number of iterations to update step by step.\n",
    "    \"\"\"\n",
    "    # Write your code here\n",
    "    \n",
    "    return theta, J_history, cost, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run gradientDescent function to train the model here\n",
    "**Note : Use the learning rate(alpha) and precision given below. Initialise theta with zeros is suggested here (but you can initialise with different numbers)**\n",
    "\n",
    "Print the final theta (learnt parameters), number of iterations to converge, value of cost function at the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fitting parameters with zeros\n",
    "theta = np.zeros((2,1))\n",
    "\n",
    "# Play with these setting to see how these parameters play a huge, for a decent converge use below parameters\n",
    "precision = 0.000001\n",
    "alpha = 0.001\n",
    "\n",
    "##################################### Your Code here ########################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Plot the Training Curve\n",
    "**Tip:** Initially the change in the loss(J) are drastic and dominating. So while plotting you may remove the first 10% of the iterations and then plot to see a smooth curve in the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Predict Outputs and Plot the results\n",
    "Same as Step 5 in the Section 2. You can use the same function predict(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a line for predictions and scatter plot for the training data X,y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 : Selecting appropriate learning rate\n",
    "**Implementation Note:** If your learning rate is too large, $J(\\theta)$ can diverge and ‘blow up’, resulting in values which are too large for computer calculations. In these situations, `numpy` will tend to return\n",
    "NaNs. NaN stands for ‘not a number’ and is often caused by undefined operations that involve −∞ and +∞. So dont worry even if you cost as inf or NaN\n",
    "\n",
    "**Repeat the training as in Section 3 with different values of alpha as listed below. Print the alpha, cost and number of iterations it took for every alpha**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros((2,1))\n",
    "precision = 0.000001\n",
    "\n",
    "# You can change this set of alphas and try out different, but keep your range less than 1\n",
    "alphas = [0.0001, 0.0003, 0.0005, 0.0007, 0.001, 0.003, 0.005, 0.007, 0.01,0.03, 0.05, 0.07, 0.1, 0.3]\n",
    "\n",
    "# Write your code here\n",
    "for alpha in alphas:\n",
    "    ......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Questions on Analytical and Gradient Descent approaches\n",
    "\n",
    "1. Are the results from both the analytical and gradient descent approach are same? If not, why?\n",
    "2. When do u prefer analytical approach and gradient descent approach? Hint: Explain interms of size of the dataset\n",
    "3. Can we use different cost function in place of MSE? If we do, where will be changes in the gradient descent step?(Just mention using words, equations are not required)\n",
    "4. How does the initialisation of the parameters $\\theta$ effects the convergence? In this case we initialised $\\theta$ to zeros. What happens if we choose closer to solution and farther to solution?\n",
    "5. Any drawbacks of Gradient descent that you could think of? Hint : Interms of Convergence rate.\n",
    "6. What if number of features increases and how does it effect the gradient descent approach and analytical approach? Note:This might require some time to look for appropriate concepts. But do note that this difference is very important to understand.\n",
    "7. Comment your observations on selecting appropriate learning rate.\n",
    "\n",
    "**Write your answers in the following Markdown**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
